{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yOkGTUF6Kp"
      },
      "source": [
        " ## Setting up a network model and starting a first training\n",
        "\n",
        "In this and the following exercise, we are going to practise, how to set up a neural network model and perform a first training with this network.\n",
        "We will use the DermaMNIST from the MedMNIST datasets, which you have seen last lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dnthk05cBlAV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: medmnist in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.24.1)\n",
            "Requirement already satisfied: torch in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.10.0)\n",
            "Requirement already satisfied: tqdm in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (4.61.2)\n",
            "Requirement already satisfied: pandas in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.2.5)\n",
            "Requirement already satisfied: fire in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.4.0)\n",
            "Requirement already satisfied: torchvision in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.11.1)\n",
            "Requirement already satisfied: scikit-image in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.19.0)\n",
            "Requirement already satisfied: Pillow in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (9.3.0)\n",
            "Requirement already satisfied: numpy in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.20.2)\n",
            "Requirement already satisfied: termcolor in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from fire->medmnist) (1.1.0)\n",
            "Requirement already satisfied: six in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from pandas->medmnist) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from pandas->medmnist) (2021.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (1.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (20.9)\n",
            "Requirement already satisfied: networkx>=2.2 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2021.7.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (1.6.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from packaging>=20.0->scikit-image->medmnist) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-learn->medmnist) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-learn->medmnist) (2.2.0)\n",
            "Requirement already satisfied: typing_extensions in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from torch->medmnist) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "# from tqdm import trange\n",
        "# from tqdm import tqdm\n",
        "# from skimage.util import montage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision as torchvision\n",
        "\n",
        "!pip install medmnist\n",
        "import medmnist\n",
        "from medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n",
        "from medmnist.evaluator import getAUC, getACC\n",
        "from medmnist.info import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MGpKZ1TFwfnL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "print(\"Version:\", medmnist.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x6nPfDx8wiAR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Info:\n",
            "{'python_class': 'DermaMNIST', 'description': 'The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.', 'url': 'https://zenodo.org/record/5208230/files/dermamnist.npz?download=1', 'MD5': '0744692d530f8e62ec473284d019b0c7', 'task': 'multi-class', 'label': {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}, 'n_channels': 3, 'n_samples': {'train': 7007, 'val': 1003, 'test': 2005}, 'license': 'CC BY-NC 4.0'}\n",
            "\n",
            "Task:\n",
            "multi-class\n",
            "\n",
            "Channels:\n",
            "3\n",
            "\n",
            "Number of classes:\n",
            "7\n",
            "\n",
            "Label:\n",
            "{'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# various MedMNIST datasets\n",
        "data_flag = 'dermamnist'\n",
        "download = True\n",
        "input_root = 'tmp_data/'\n",
        "!mkdir 'tmp_data'\n",
        "\n",
        "flag_to_class = {\n",
        "    \"pathmnist\": PathMNIST,\n",
        "    \"chestmnist\": ChestMNIST,\n",
        "    \"dermamnist\": DermaMNIST,\n",
        "    \"octmnist\": OCTMNIST,\n",
        "    \"pneumoniamnist\": PneumoniaMNIST,\n",
        "    \"retinamnist\": RetinaMNIST,\n",
        "    \"breastmnist\": BreastMNIST,\n",
        "    \"organmnist_axial\": OrganMNISTAxial,\n",
        "    \"organmnist_coronal\": OrganMNISTCoronal,\n",
        "    \"organmnist_sagittal\": OrganMNISTSagittal,\n",
        "}\n",
        "\n",
        "DataClass = flag_to_class[data_flag]\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "label_dict = info['label']\n",
        "\n",
        "print(f\"Info:\\n{info}\\n\")\n",
        "print(f\"Task:\\n{task}\\n\")\n",
        "print(f\"Channels:\\n{n_channels}\\n\")\n",
        "print(f\"Number of classes:\\n{n_classes}\\n\")\n",
        "print(f\"Label:\\n{label_dict}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2I6F49b8fRx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Defining the augmentations\n",
        "\n",
        "As described in the last exercise, we now define the augmentations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HjF-Aq9BF6K_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Imagenet values\n",
        "norm_mean = (0.4914)\n",
        "norm_std = (0.2023)\n",
        "\n",
        "# define the transformaitons the images go through each time it is used for training\n",
        "# includes augmentation AND normalization as described above\n",
        "augmentation_train = transforms.Compose([\n",
        "                                  # resize image to the network input size\n",
        "                                  transforms.Resize((28,28)),\n",
        "                                  # rotate the image with a certain angle range, randomly chosen\n",
        "                                  transforms.RandomRotation(degrees=20),\n",
        "                                  # convert the image into a tensor so it can be processed by the GPU\n",
        "                                  transforms.ToTensor(),\n",
        "                                  # normalize the image with the mean and std of ImageNet\n",
        "                                  transforms.Normalize(norm_mean, norm_std),\n",
        "                                   ])\n",
        "\n",
        "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
        "augmentation_test = transforms.Compose([\n",
        "                    transforms.Resize((28,28)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(norm_mean, norm_std),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGR6pIuMJsqK"
      },
      "source": [
        "## Splitting up data\n",
        "\n",
        "Set up datasets for training, validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AHYTu04_yDvo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://zenodo.org/record/5208230/files/dermamnist.npz?download=1 to tmp_data/dermamnist.npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19725312it [00:16, 1169703.20it/s]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n",
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "train_dataset = DataClass(root=input_root, split='train', transform=augmentation_train, download=download)\n",
        "test_dataset = DataClass(root=input_root, split='test', transform=augmentation_test, download=download)\n",
        "val_dataset = DataClass(root=input_root, split='val', transform=augmentation_test, download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zL9p3c52xpUB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 7007\n",
            "    Root location: tmp_data/\n",
            "    Split: train\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 1003\n",
            "    Root location: tmp_data/\n",
            "    Split: val\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 2005\n",
            "    Root location: tmp_data/\n",
            "    Split: test\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n"
          ]
        }
      ],
      "source": [
        "# Some detailed information about all splits\n",
        "print(\"===================\")\n",
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(val_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZXPqSvKNyh"
      },
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "PyTorch provides template Dataloader classes for easy data handling, assigning according transforms and splits. You can find more information about how PyTorch handles datasets and data loading [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VUURp9J01EtJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "### encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NMfm-GkQyFe3"
      },
      "outputs": [],
      "source": [
        "### the next() function returns the next item from the iterator.\n",
        "batch_images, batch_labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7O9BTWgOyIhK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb6dd6c8e20>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEvCAYAAAAtufaDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAY5UlEQVR4nO3dW4yc93nf8d8zxz1wl1ySIs1QcnSwYlhIESlh1RRqC7euA8c3sgMkiFAYKhBAvogBG8hFDQNBfJNUKHxoLwIXci1ELRwHQW3HunDbyIYBJ4hhmFYUWyrrSFYomyJFmuJhucs9zOHpBUcGLXP1/3FnuMN//P0AApfDR+/83/ed+e3M7PM+G5kpAKhVY9oLAIBxEGIAqkaIAagaIQagaoQYgKoRYgCq1trJO+tEN2c0v5N3idfZuGPWqms2hxO7z9lW37vP8O5zmFGs2RhO9qHdMtfm7IO7ts2euQ+b5eNhLl9hdly1X1n1Cifoks6fzcxbXn/7WGc6It4l6b9Iakr6b5n56BvVz2he/yzeMc5dYkzf/8P7rLrFhcsTu8979p+26va016w6JwReWN5vbathPmuXut7xWGhvFGuOX9prbeulU/usuuapbrGmfakcdJIUA6tMt/7Hv/EKJ+gr+T9futbt2347GRFNSX8s6dcl3SPpoYi4Z7vbA4DtGOczsfslvZCZL2bmpqQ/k/TgZJYFAJ5xQuywpB9e9fcTo9sAYMeM85nYtd5k/9QHDBHxiKRHJGlGc2PcHQD8tHFeiZ2QdNtVf79V0snXF2XmY5l5JDOPtFX+ABIArsc4IfYtSXdHxB0R0ZH025KenMyyAMCz7beTmdmPiA9I+j+60mLxeGY+N7GVAYBhrD6xzPyypC9PaC0AcN12tGP/Z8W9f+vVfeXEL1h1584sFmvml7xG0bnGulW3sjpj1e3dXe7cPrvuXaWxv7ti1a0OOsWai2ve+jf73lPg5GC3VRdG8+zmpnefuTKFp6fXE3tT4dpJAFUjxABUjRADUDVCDEDVCDEAVSPEAFSNEANQNUIMQNX+0Te7vvVo26obppfn3mhhb3Tv3jmvQXV1oXzhfLfds7bVGzStun7PqxsMJ/d98O/OeZOcWo3yrGV3Pzc3vKdA77w3vCD65W7RnDPHp5qyWW6w7S142xp2zPnUNxFeiQGoGiEGoGqEGICqEWIAqkaIAagaIQagaoQYgKoRYgCqRogBqNqOduzPvk36xc+Wc3NXa6NY0xt6HdkvXvZalftm53mnUe62vqVzydrWhbVZq67fL+/r6prXUd7vmaf8fHkEtCSda5RHT1/e8K6aOLDojad+Zbl8Tue7m9a2nGMrSd71EFIMyh37jYvmeOqD5eeBJOVG+dw3+tam1Fr15lO/+Og/t+ru/PA3vDseA6/EAFSNEANQNUIMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUbUc79ld6Xf3VK3cV65rGDPWlGW8+/fFX91p1vU3vUDRb5Y7903u9qwQuLM9ZdYPL5bUNel6HfWPd+77V2vA6t3VipliytuCt7QdrXl1eKNetmHPs9x9Y9u5zv1WmwYu7ykXmS4fh0DsHTlXvgHfNQax4VzA0eubjYwfwSgxA1QgxAFUjxABUjRADUDVCDEDVCDEAVSPEAFSNEANQNUIMQNV2tGO/v9nUj15aKtY5c8rP7PFmqMcZb/a8q290Ub941pud3zL3oTlXHpA+WPXm2Ef5YojrYv2qg77X3T3c8LrF514u1/UWve/PZ7Vo1ck9bkvGMHuz2b07Y3bZ31quW1/2ngeNfd5jst0xh/bvgLFCLCKOS7okaSCpn5lHJrEoAHBN4pXYv87MsxPYDgBcNz4TA1C1cUMsJf1lRHw7Ih65VkFEPBIRRyPi6GBldcy7A4CfNO7byQcy82REHJD0VET8v8z8+tUFmfmYpMckqfvzt+aY9wcAP2GsV2KZeXL05xlJX5R0/yQWBQCubYdYRMxHxMJrX0v6NUnPTmphAOAY5+3kQUlfjIjXtvOnmfm/J7IqADBtO8Qy80VJvzTBtfxY83L5BWJjuTwWWZKGXg/olR9ROGVNo7DrdUb2l71xzM7anAZhd1uSFGYvY9Ooa6+aL/hfNUdsGz2gaR6OaHrnKjregWucLD8uw5ucrf7yvFU3mC/vg/OckqTBonfg0mz+feF/3Fesecv7/tbb2BZosQBQNUIMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUjRADULUdHU8dvdDMK+W77M+Xu6P7c14H9XDGbC1252vMGu3WPe97w8wp8/Abaxt2vR1I89tWa9Xr3G5ulGs6y97aevPmGGtj0rLT1S9JgxXvko5hmMfXOA/u2hrmWO/WmfLjqLdoPg8a3n7O7TJOvKTZjrmzY+CVGICqEWIAqkaIAagaIQagaoQYgKoRYgCqRogBqBohBqBqhBiAqu1sx/5A6iyX6/q7yjXDeXdQuTlsfYJx3rzoHVan213yOrz77ox9s6xz0atrbpY7vN2rBBrmXP8wms9bTbP7v+0tzr4iwpjFPzAfu4O+t7bmpWaxZv4O74QeXFjx6mYvWXXzrfKD/Li1pa3xSgxA1QgxAFUjxABUjRADUDVCDEDVCDEAVSPEAFSNEANQtR1tds2WtL6v3Aw4NBoGNeHmTqdhUJK0auS+OeJ3fb9X1321fJ9Nd+SxWec0sUqyjm+ah9atC6NXdNgx77M9uf2UpDTGl3fN0c5h3mdvsXzg9s1ftra11PXqumZncn9ontQx8EoMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUjRADUDVCDEDVdrZjvzvU4M71cp3Tjb9ijoC+7OV0trzO7dZKeW0N92oCs1ncGendXjU3ZpYN22ad0ZDtdNhLUnvdW1x/zjy+jqG3rZwxZmJL1jj0jeWut62m+Zjslg/wyoZ3n7Mt75KOljMj3HTXt2asuq8cufbtxWd4RDweEWci4tmrbtsbEU9FxPOjP5fM9QLARDkvU/5E0rted9uHJX01M++W9NXR3wFgxxVDLDO/Lunc625+UNITo6+fkPSeyS4LADzb/WD/YGaekqTRnwcmtyQA8N3wn05GxCMRcTQijg4urd7ouwPwM2a7IXY6Ig5J0ujPM1sVZuZjmXkkM480F+a3eXcAcG3bDbEnJT08+vphSV+azHIA4Po4LRafk/QNSW+NiBMR8TuSHpX0zoh4XtI7R38HgB1X7BjNzIe2+Kd3THgtAHDddrRjXxkabJbfwTba5W7g5p5N6y77s+aM73Wvrtcorz+GXqf17Gnv3fzAaLZurVmb8jv2zUdGb7Hcod5a8bbV6HmLc+bnN7yHh3/c1rxztbm7fDzCPAeNjcn9HolXu15X/Plb5qy6Ow6+atW9dXHLj8t/bF/HfIBsgWsnAVSNEANQNUIMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUbWc79kMKY274cK28rGHDbHs2NRe92eLDC+V28caq972hP2uVKYxm60F3gnPnJasLXJJ6i+Xz0Jv3NtbY9Oqc8e79eXM+/ao5Y9+88KO5Ua4ZzHvz6Qfm732Qs7n09nOwbFwOIWljnxcd43bjO3glBqBqhBiAqhFiAKpGiAGoGiEGoGqEGICqEWIAqkaIAajazja7DqXcMLsGC5ozg4ls5zXtTt+q21goNw0O+l7DoDtC2Wl5zJhss+uw4zVaOvvgNKdK0mDGHE/dNorMb889s6HU7BW1jltjyeiIlTQ359UNBuWdXVs1ZpxLOrh/2aq7ddcFq26Q5bW1Y7znMq/EAFSNEANQNUIMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUbWc79iXJGSttlAz7Xgt1q+N1A2+8MmfVxaB8v24D8sBrolbD2V6Yo4zNMrfLfmhcnNDYdO/UK3O+9aYxBl2S+ovmjrbNOuPx/W/ufMHa1HzL69g/uba7WHPZvIpkpumNaV8feNFxrjdfrNnbXrW2tRVeiQGoGiEGoGqEGICqEWIAqkaIAagaIQagaoQYgKoRYgCqRogBqNqOduw3WkPN7Vkr1vX75Tn8vXVv6b2LXlt8c93Lc6cx3p3H7nbF93aXW/adKwkkSWad3bE/Z6xtwzu2jZ63NmNsu9Kcna/uhH9Xw0z5dzUcX9lrbeufLJ206v7pnpeKNT9c9+7T9XPdC1bdTKN8BcC69UsTtlZ8OETE4xFxJiKeveq2j0bEyxHxzOi/d4+1CgDYJudb5J9Ietc1bv9kZt47+u/Lk10WAHiKIZaZX5d0bgfWAgDXbZwP9j8QEd8Zvd1c2qooIh6JiKMRcbS/fHmMuwOAn7bdEPuUpLsk3SvplKSPb1WYmY9l5pHMPNJa9MbdAIBrWyGWmaczc5CZQ0mflnT/ZJcFAJ5thVhEHLrqr++V9OxWtQBwIxWbrSLic5LeLml/RJyQ9AeS3h4R9+rKnNDjkt5/45YIAFsrhlhmPnSNmz9zA9byY81mudPS7ItUmGOsXYMZowvU7HY1+gCvMEYjZ8PcT7Ov0G3YddbWecW702HXHCl9qDy2+dDBC9a2Tr9aHu0sSUu7vRHKrWa5efZHK+WRzZL0Ynu/VTe3uFmscUddX+rPWHUvmc2zQ6MzedUcnb0VLjsCUDVCDEDVCDEAVSPEAFSNEANQNUIMQNUIMQBVI8QAVI0QA1C1HR1PLUlhdIL3NsvjqVttb6xwb9bL6aE5jrl9sby93l5vY8M9Zsv+0Dho7hUMq+VjK0na7a0t++XjsbnHOx65qzzaWZJu2X+pWLPQ8TrUh3uXrbqlmfJYdUk6vz5brJnvljvsJanT8I7H6Y1Fq87hjs6eb092H8bBKzEAVSPEAFSNEANQNUIMQNUIMQBVI8QAVI0QA1A1QgxA1QgxAFXb0Y799vfXdfg3nivWLf+vu4o155a9OeWx4u1i95yX5xsHylcKZMds/ze63a9ssFzSnPWuYBgMzNb+weS+vx24+6xVN9v2rhLoNLx9ddwy583ObzgnQVLXmLH/K/t+YG1rtukdj6fP31asOb2yy9pWmr9c4bzKVyZI0j37TxdrDs9esLa1FV6JAagaIQagaoQYgKoRYgCqRogBqBohBqBqhBiAqhFiAKpGiAGo2o7P2Hc0otwd3Wh4HdQ553V3bzTN7Rlrk7m25qw3f7zTKdetLc9Y24oZ73h0Zrxu8dluue7Ni+etbW0OvIfjhY1yt3iz4V01sbd72apz7WqXZ/uf63lXm6ytt626wbD8WmT50py3LfMqkkMHLlh1i+318rY63ra2wisxAFUjxABUjRADUDVCDEDVCDEAVSPEAFSNEANQNUIMQNVuymbX088dKNZk09tWmNOYZTa7qlWum91dbvC7HpubxmnqezuaRmOkJGXXPXBl3z+/z6qb73gNtm1jBHQrvGbXPZ01q2655zUTt43R2RtmU+/3zpafB5J0eb1TrHHHTrtNzvtmvSbhX9l1vFhz74w3rnsrxUd0RNwWEV+LiGMR8VxEfHB0+96IeCoinh/9uTTWSgBgG5xvy31Jv5eZb5P0q5J+NyLukfRhSV/NzLslfXX0dwDYUcUQy8xTmfn06OtLko5JOizpQUlPjMqekPSeG7RGANjSdX2wHxG3S7pP0jclHczMU9KVoJPkvYEHgAmyQywidkn6vKQPZebydfx/j0TE0Yg42lP5Cn8AuB5WiEVEW1cC7LOZ+YXRzacj4tDo3w9JOnOt/zczH8vMI5l5pK3uJNYMAD/m/HQyJH1G0rHM/MRV//SkpIdHXz8s6UuTXx4AvDGnYeUBSe+T9N2IeGZ020ckPSrpzyPidyT9QNJv3pAVAsAbKIZYZv61pK065d4x2eUAwPW5KTv2m5cn1y3eMrfV3PS2l1G+VODyreblBPsn94OOhjnqutnyOtnnZswDYrht8aJV12ma47qNrvj5lndsT1zeY9X9w7m9Vt1tey4Ua1rm6GxnW5K01i+Psd5tXpkwNDv7f+vgUavu386dKNbMhTeGeytcOwmgaoQYgKoRYgCqRogBqBohBqBqhBiAqhFiAKpGiAGoGiEGoGo3Zcf+7b//jWLN9z/2q9a23I791mVvxv763vL2Ohe87w39TW9ue3+x3KHeXPBmozeaXre4a89cuRP89l2vWttyu8UXW+XfYeBu69j5N1l1G9/bbdVdvLe8tvv2vWxtq5fe4+hib7ZYM9v0Hh+uW1reNK5Lw/LzambMxySvxABUjRADUDVCDEDVCDEAVSPEAFSNEANQNUIMQNUIMQBVuymbXR3tZS9/h+bk2/V9XnPkYKbcvDfsePc5mDOb/Nrlumar3BArSb1N75S7dU6z67ELXkPpcMtf5fCT3jx/3qqztrXgbevn/qU3YvvCRrnx9NzmnLWtd+w9ZtUNjNciG+YTodvwmmLf2vaOx8Fm+XiMi1diAKpGiAGoGiEGoGqEGICqEWIAqkaIAagaIQagaoQYgKoRYgCqVm3HfqPv1ZlTipUTPBL9Pd7i2gubVl2zVe7Yb5kd+wf3XLLqTp7dY9Udf/5gsaazrzyyWZKWFi5bda+25os1cy3v2N45f9aqa4Z3dcVgvvy64GDbG+18uO1dTdCO8uOtKW/8+kLDO1cz4T2xhioft26Yl9VsgVdiAKpGiAGoGiEGoGqEGICqEWIAqkaIAagaIQagaoQYgKoRYgCqVm3H/q1/9DdW3d//1/utusaal+fNdaNT2bxKoHexa9X1Z8sd2a1Fr2P/wtqMVTdYNruoZ8zfE2A4ML9i1S11y539u9vl2f+SdEf3R1bdnqZ3NUEvm8WaucaGta2B+0AyLjeZaXjHY8Gcsf/vbnvAqpusF655a/GZGxG3RcTXIuJYRDwXER8c3f7RiHg5Ip4Z/ffuCa8YAIqcV2J9Sb+XmU9HxIKkb0fEU6N/+2RmfuzGLQ8A3lgxxDLzlKRTo68vRcQxSYdv9MIAwHFdH+xHxO2S7pP0zdFNH4iI70TE4xGxNOnFAUCJHWIRsUvS5yV9KDOXJX1K0l2S7tWVV2of3+L/eyQijkbE0Z68DzQBwGWFWES0dSXAPpuZX5CkzDydmYPMHEr6tKRr/hgwMx/LzCOZeaQt76dxAOByfjoZkj4j6VhmfuKq2w9dVfZeSc9OfnkA8Macn04+IOl9kr4bEc+MbvuIpIci4l5JKem4pPffgPUBwBtyfjr517p2++aXJ78cALg+1Xbsu/YevmDVrW96Herra51izS1LXuf5pcte93y/V+4C7xk1knTPm16x6v4hvJnsjUa5Y3+m7f3OgVu63nHb0y53zx/uXrC25Zo3u+xXh+XPfYfmz9P++C2/YNX9rOPaSQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUjRADUDVCDEDV/tE3u+6f88YKN3d5Y5Z7w3JTaTO8bS10vQbKYZbHFHca3njqTtOrO/KmH1p1/Sx/H3SOmSTNNr3RyN1GuXn2bG+Xta12eMfjk295m1WHnccrMQBVI8QAVI0QA1A1QgxA1QgxAFUjxABUjRADUDVCDEDVCDEAVYtMbwzxRO4s4keSXnrdzfslnd2xRUxe7euX6t+H2tcv1b8PO7H+n8/MW15/446G2LVExNHMPDLVRYyh9vVL9e9D7euX6t+Haa6ft5MAqkaIAajazRBij017AWOqff1S/ftQ+/ql+vdhauuf+mdiADCOm+GVGABs29RCLCLeFRHfi4gXIuLD01rHOCLieER8NyKeiYij016PIyIej4gzEfHsVbftjYinIuL50Z9L01zjG9li/R+NiJdH5+GZiHj3NNf4RiLitoj4WkQci4jnIuKDo9trOgdb7cNUzsNU3k5GRFPS30t6p6QTkr4l6aHM/L87vpgxRMRxSUcys5r+noj4V5JWJP33zPzF0W3/SdK5zHx09A1lKTP/wzTXuZUt1v9RSSuZ+bFprs0REYckHcrMpyNiQdK3Jb1H0r9XPedgq334LU3hPEzrldj9kl7IzBczc1PSn0l6cEpr+ZmSmV+XdO51Nz8o6YnR10/oygPyprTF+quRmacy8+nR15ckHZN0WHWdg632YSqmFWKHJV09xP2EpngQxpCS/jIivh0Rj0x7MWM4mJmnpCsPUEkHprye7fhARHxn9Hbzpn0rdrWIuF3SfZK+qUrPwev2QZrCeZhWiF3rN1/U+GPSBzLzlyX9uqTfHb3Vwc77lKS7JN0r6ZSkj091NYaI2CXp85I+lJnL017PdlxjH6ZyHqYVYick3XbV32+VdHJKa9m2zDw5+vOMpC/qytvkGp0efc7x2ucdZ6a8nuuSmaczc5CZQ0mf1k1+HiKirStP/s9m5hdGN1d1Dq61D9M6D9MKsW9Jujsi7oiIjqTflvTklNayLRExP/pQUxExL+nXJD37xv/XTetJSQ+Pvn5Y0pemuJbr9tqTf+S9uonPQ0SEpM9IOpaZn7jqn6o5B1vtw7TOw9SaXUc/fv3PkpqSHs/MP5zKQrYpIu7UlVdf0pXf3/mnNexDRHxO0tt1ZerAaUl/IOkvJP25pDdL+oGk38zMm/LD8y3W/3ZdeQuTko5Lev9rny/dbCLiX0j6K0nflfTaLyj9iK58plTLOdhqHx7SFM4DHfsAqkbHPoCqEWIAqkaIAagaIQagaoQYgKoRYgCqRogBqBohBqBq/x8vupOaOntlxAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# show all 3 channels of image 10\n",
        "print(batch_images[0].shape)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][0,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][1,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][2,:,:])\n",
        "\n",
        "### different color maps\n",
        "### cmap='bone', cmap = 'summer', cmap = 'seismic'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3QclrDwU6j2B"
      },
      "source": [
        "## Define a Convolutional Neural Network\n",
        "\n",
        "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from [torch library](https://pytorch.org/docs/stable/nn.html).\n",
        "\n",
        "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authors’ implementation of LeNet was used primarily for OCR and character recognition in documents. The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs.\n",
        "\n",
        "To define a neural network in PyTorch one has to create a class inhereting from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Crucially, this class has to include the function forward() which defines which computation should be performed at every call given a batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C4SswIxD6j2B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = 7\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), padding=2)\n",
        "        self.nonlin1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5)) \n",
        "        self.nonlin2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.nonlin3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.nonlin4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "        self.nonlin5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.nonlin1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.nonlin2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.nonlin3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.nonlin4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.nonlin5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        \n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use torchsummary to print out the architecture of your model given a certain input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [127, 6, 28, 28]             156\n",
            "              ReLU-2           [127, 6, 28, 28]               0\n",
            "         MaxPool2d-3           [127, 6, 14, 14]               0\n",
            "            Conv2d-4          [127, 16, 10, 10]           2,416\n",
            "              ReLU-5          [127, 16, 10, 10]               0\n",
            "         MaxPool2d-6            [127, 16, 5, 5]               0\n",
            "            Linear-7                 [127, 120]          48,120\n",
            "              ReLU-8                 [127, 120]               0\n",
            "            Linear-9                  [127, 84]          10,164\n",
            "             ReLU-10                  [127, 84]               0\n",
            "           Linear-11                   [127, 7]             595\n",
            "             ReLU-12                   [127, 7]               0\n",
            "================================================================\n",
            "Total params: 61,451\n",
            "Trainable params: 61,451\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.38\n",
            "Forward/backward pass size (MB): 14.15\n",
            "Params size (MB): 0.23\n",
            "Estimated Total Size (MB): 14.77\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = LeNet()\n",
        "model.to(device)\n",
        "\n",
        "summary(model, input_size=(1,28,28), batch_size=127)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_R_SSWF6Lb"
      },
      "source": [
        "## Define a Loss function\n",
        "\n",
        "Let's use a Classification Cross-Entropy loss.\n",
        "\n",
        "$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$\n",
        "\n",
        "### Median Frequency Balancing\n",
        "There are datasets which have a large imbalance in the amount of label occurrence. A prediction would be therefore biased towards stronger represented classes. As a solution, we use **Median Frequency Balancing**. Essentially this tasks the optimizer to weight examples of rare cases more highly than examples of common cases that are processed more frequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class 0 : 1.5745614035087718\n",
            "class 1 : 1.0\n",
            "class 2 : 0.4668400520156047\n",
            "class 3 : 4.4875\n",
            "class 4 : 0.46084724005134786\n",
            "class 5 : 0.07649691029192414\n",
            "class 6 : 3.6262626262626263\n"
          ]
        }
      ],
      "source": [
        "# get the class labels of each image\n",
        "class_labels = train_dataset.labels\n",
        "# get the number of different classes\n",
        "num_classes = np.max(class_labels)+1\n",
        "# empty array for counting instance of each class\n",
        "count_labels = np.zeros(num_classes)\n",
        "# empty array for weights of each class\n",
        "class_weights = np.zeros(num_classes)\n",
        "\n",
        "# populate the count array\n",
        "for l in class_labels:\n",
        "    count_labels[l] += 1\n",
        "\n",
        "# get median count\n",
        "median_freq = np.median(count_labels)\n",
        "\n",
        "# calculate the weigths\n",
        "for i in range(num_classes):\n",
        "    class_weights[i] = median_freq/count_labels[i]\n",
        "\n",
        "# print the weights\n",
        "for i in range(num_classes):\n",
        "    print(\"class\", i, \":\", class_weights[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pPUGMq3NCnP"
      },
      "source": [
        "Now we define the loss function with the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j-Q72RJ-M84P"
      },
      "outputs": [],
      "source": [
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRBdC31sF6Li"
      },
      "source": [
        "## Define the Optimizer\n",
        "\n",
        "The most common and effective optimizer currently used is **Adam: Adaptive Moments**. You can check out [the paper on it](https://arxiv.org/abs/1412.6980) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pmMjYnCjF6Lk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (nonlin1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (nonlin2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (nonlin3): ReLU()\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (nonlin4): ReLU()\n",
            "  (fc3): Linear(in_features=84, out_features=7, bias=True)\n",
            "  (nonlin5): ReLU()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# now lets go back to the initial LeNet architecture\n",
        "net = LeNet()\n",
        "net = net.to(device)\n",
        "\n",
        "# and define an optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oGr-AfqF6Ln"
      },
      "source": [
        "## Network training\n",
        "\n",
        "After everything has been set up, we can now start an actual training on our dataset. To save time, for the moment we will run only ten epochs. Within the training, our dataloader is used to load a batch from our dataset. This batch is forwarded to the model. The corresponding output is compared against its labels with the chosen loss function, here called 'criterion'. Then, the loss values are backpropagated through the whole model.\n",
        "\n",
        "After, the training step a validation step is performed. Here the network is set to .eval() mode in which its weights are not being updated and consequently backprogagation is not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, training loss: 0.0153, training accuracy: 2.8000000000000003%, validation loss: 0.0155, validation accuracy: 2.6%, \n",
            "Epoch: 2, training loss: 0.0153, training accuracy: 1.6%, validation loss: 0.0155, validation accuracy: 1.6%, \n",
            "Epoch: 3, training loss: 0.0153, training accuracy: 1.7000000000000002%, validation loss: 0.0155, validation accuracy: 1.7000000000000002%, \n",
            "Epoch: 4, training loss: 0.0152, training accuracy: 2.0%, validation loss: 0.0155, validation accuracy: 2.1%, \n",
            "Epoch: 5, training loss: 0.0152, training accuracy: 3.0%, validation loss: 0.0155, validation accuracy: 3.0%, \n",
            "Epoch: 6, training loss: 0.0152, training accuracy: 4.0%, validation loss: 0.0155, validation accuracy: 3.5999999999999996%, \n",
            "Epoch: 7, training loss: 0.0152, training accuracy: 5.4%, validation loss: 0.0155, validation accuracy: 4.9%, \n",
            "Epoch: 8, training loss: 0.0152, training accuracy: 6.7%, validation loss: 0.0155, validation accuracy: 5.6000000000000005%, \n",
            "Epoch: 9, training loss: 0.0152, training accuracy: 7.8%, validation loss: 0.0155, validation accuracy: 6.2%, \n",
            "Epoch: 10, training loss: 0.0152, training accuracy: 9.2%, validation loss: 0.0154, validation accuracy: 7.3%, \n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ##### Training loop #####\n",
        "    running_loss_train = 0.0\n",
        "    num_correct_train = 0\n",
        "    num_all_train = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), torch.squeeze(labels).to(device)\n",
        "\n",
        "        # set model to training mode\n",
        "        net.train()\n",
        "\n",
        "        # set the parameter gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # calculate the network output and its loss \n",
        "        outputs = net(inputs[:,0:1,:,:])\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # During training we need to backpropagate the loss and conduct an optimization step \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute running loss and accuracy\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        num_correct_train += torch.sum(predicted == labels).item()\n",
        "        num_all_train += labels.size()[0]\n",
        "\n",
        "        training_loss = running_loss_train / num_all_train\n",
        "        training_accuracy = num_correct_train / num_all_train\n",
        "\n",
        "\n",
        "    ##### Validation loop #####\n",
        "    running_loss_val = 0.0\n",
        "    num_correct_val = 0\n",
        "    num_all_val = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), torch.squeeze(labels).to(device)\n",
        "\n",
        "        # set model to evaluation mode\n",
        "        net.eval()\n",
        "\n",
        "        # calculate the network output and its loss \n",
        "        outputs = net(inputs[:,0:1,:,:])\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # compute running loss and accuracy\n",
        "        running_loss_val += loss\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        num_correct_val += torch.sum(predicted == labels).item()\n",
        "        num_all_val += labels.size()[0]\n",
        "\n",
        "        validation_loss = running_loss_val / num_all_val\n",
        "        validation_accuracy = num_correct_val / num_all_val\n",
        "\n",
        "    print('Epoch: {}, training loss: {:.4f}, training accuracy: {}%, validation loss: {:.4f}, validation accuracy: {}%, '.format(epoch+1, training_loss, np.round(training_accuracy, 3) * 100, validation_loss, np.round(validation_accuracy, 3) * 100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIbJukXZA7ap"
      },
      "source": [
        "## Homework\n",
        "After performing the training and validation of your system you are now ready to perform the inference on your test set. Implement the inference step for out dataset. Hint: It will look very similar to one the previous loops (train or validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "Loss for test set is 0.01585624872003113\n",
            "Test accuracy of the network: 639.6%\n"
          ]
        }
      ],
      "source": [
        "running_loss_test = 0.0\n",
        "num_correct_test = 0\n",
        "num_all_test = 0\n",
        "\n",
        "for i, data in enumerate(test_loader):\n",
        "\n",
        "    ########## Implement the data loading and prediction on the test set ##########\n",
        "\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    print(type(labels))\n",
        "    net.eval()  # Set model to validation mode\n",
        "\n",
        "    # -------------------------------\n",
        "    # Calculate the network output and its loss\n",
        "    outputs = net(inputs[:,0:1,:,:])\n",
        "    loss = criterion(outputs, torch.argmax(labels, 1))\n",
        "\n",
        "    ########## ------------------------------- ##########\n",
        "    \n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    num_correct_test += torch.sum(predicted == labels).item()\n",
        "    num_all_test += labels.size()[0]\n",
        "    running_loss_test += loss.item()\n",
        "\n",
        "    test_loss = running_loss_test / num_all_test\n",
        "    test_accuracy = num_correct_test / num_all_test\n",
        "\n",
        "print('Loss for test set is {}'.format(test_loss))\n",
        "print('Test accuracy of the network: {}%'.format(np.round(test_accuracy, 3)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Exercise_7.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('default')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "62f47e0a940d9130cb9d8ad31b00082fdc26fbada418265db41289562a6ad16c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
